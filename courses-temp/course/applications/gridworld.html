<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Gridworld Agent (Pathfinding) - DRL Course</title>
    <link rel="stylesheet" href="../../static/drl-course-css.css">
    <link rel="stylesheet" href="../../static/applications.css">
    <style>
        .code-concept {
            background-color: #f8f9fa;
            border-radius: 8px;
            border-left: 4px solid #3498db;
            margin: 1.5rem 0;
            overflow-x: auto;
        }

        .code-concept pre {
            margin: 0;
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            font-size: 0.95rem;
            line-height: 1.5;
        }

        @media screen and (max-width: 768px) {
            .code-concept {
                border-radius: 4px;
            }
            
            .code-concept pre {
                font-size: 0.85rem;
            }
        }
    </style>
</head>

<body>
    <header>
        <h1>Deep Reinforcement Learning: Project</h1>
        <p>Building a Gridworld Pathfinding Agent</p>
    </header>
    
    <nav>
        <ul>
            <li><a href="../../index.html">Home</a></li>
            <li class="dropdown">
                <a href="../Q-learning.html">Q-Learning</a>
                <ul class="submenu">
                    <li><a href="../Q-learning.html#introduction">Introduction</a></li>
                    <li><a href="../Q-learning.html#rl-basics">RL Basics</a></li>
                    <li><a href="../Q-learning.html#q-learning">Q-Learning Algorithm</a></li>
                </ul>
            </li>
            <li><a href="../deep-q-learning.html">Deep Q-Learning</a></li>
            <li><a href="../applications.html" class="active">Applications</a></li>
        </ul>
    </nav>
    
    <main>
        <section class="project-container">
            <div class="project-banner">
                <h2>Gridworld Agent (Pathfinding)</h2>
                <p>Implementing a basic reinforcement learning agent to navigate through a grid environment</p>
            </div>
            
            <div class="project-meta">
                <div class="meta-item">
                    <span class="meta-icon">üîç</span> Difficulty: Beginner
                </div>
                <div class="meta-item">
                    <span class="meta-icon">‚öôÔ∏è</span> Algorithm: Q-Learning
                </div>
                <div class="meta-item">
                    <span class="meta-icon">‚è±Ô∏è</span> Estimated Time: 2-3 hours
                </div>
                <div class="meta-item">
                    <span class="meta-icon">üìö</span> Prerequisites: Basic Python, RL Fundamentals
                </div>
            </div>
            
            <h3>Project Overview</h3>
            <p>In this project, we'll create a Q-learning agent that learns to navigate through a grid-based environment. The agent will discover the optimal path from a starting position to a goal position while avoiding obstacles. This project serves as an excellent introduction to reinforcement learning concepts, demonstrating how an agent can learn through trial and error.</p>
            
            <div class="note">
                <p><strong>Key Learning Objectives:</strong></p>
                <ul>
                    <li>Understanding the Q-learning algorithm and its components</li>
                    <li>Creating a grid-based environment with rewards and penalties</li>
                    <li>Implementing exploration vs. exploitation strategies</li>
                    <li>Visualizing the learning process and resulting policy</li>
                    <li>Understanding how hyperparameters affect learning performance</li>
                </ul>
            </div>
            
            <h3>The Environment</h3>
            <p>Our gridworld environment consists of a rectangular grid where:</p>
            <ul>
                <li>The agent starts at a designated start cell (S)</li>
                <li>The goal is to reach a target cell (G)</li>
                <li>Some cells are walls/obstacles that cannot be traversed</li>
                <li>The agent can move in four directions: up, down, left, right</li>
                <li>Actions have deterministic outcomes (no randomness in transitions)</li>
            </ul>
            
            <div class="environment-diagram">
                <svg width="320" height="320" viewBox="0 0 320 320">
                    <!-- Grid lines -->
                    <g stroke="#ccc" stroke-width="1">
                        <!-- Vertical lines -->
                        <line x1="0" y1="0" x2="0" y2="320" />
                        <line x1="64" y1="0" x2="64" y2="320" />
                        <line x1="128" y1="0" x2="128" y2="320" />
                        <line x1="192" y1="0" x2="192" y2="320" />
                        <line x1="256" y1="0" x2="256" y2="320" />
                        <line x1="320" y1="0" x2="320" y2="320" />
                        
                        <!-- Horizontal lines -->
                        <line x1="0" y1="0" x2="320" y2="0" />
                        <line x1="0" y1="64" x2="320" y2="64" />
                        <line x1="0" y1="128" x2="320" y2="128" />
                        <line x1="0" y1="192" x2="320" y2="192" />
                        <line x1="0" y1="256" x2="320" y2="256" />
                        <line x1="0" y1="320" x2="320" y2="320" />
                    </g>
                    
                    <!-- Obstacles -->
                    <rect x="64" y="64" width="64" height="64" fill="#333" />
                    <rect x="128" y="64" width="64" height="64" fill="#333" />
                    <rect x="128" y="128" width="64" height="64" fill="#333" />
                    
                    <!-- Start position -->
                    <rect x="0" y="0" width="64" height="64" fill="#FFD700" />
                    <text x="32" y="38" font-size="24" text-anchor="middle" font-weight="bold">S</text>
                    
                    <!-- Goal position -->
                    <rect x="192" y="192" width="64" height="64" fill="#2ecc71" />
                    <text x="224" y="230" font-size="24" text-anchor="middle" font-weight="bold">G</text>
                    
                    <!-- Optimal path visualization -->
                    <rect x="0" y="64" width="64" height="64" fill="rgba(52, 152, 219, 0.3)" />
                    <text x="32" y="102" font-size="20" text-anchor="middle">‚Üì</text>
                    
                    <rect x="0" y="128" width="64" height="64" fill="rgba(52, 152, 219, 0.3)" />
                    <text x="32" y="166" font-size="20" text-anchor="middle">‚Üì</text>
                    
                    <rect x="0" y="192" width="64" height="64" fill="rgba(52, 152, 219, 0.3)" />
                    <text x="32" y="230" font-size="20" text-anchor="middle">‚Üí</text>
                    
                    <rect x="64" y="192" width="64" height="64" fill="rgba(52, 152, 219, 0.3)" />
                    <text x="96" y="230" font-size="20" text-anchor="middle">‚Üí</text>
                    
                    <rect x="128" y="192" width="64" height="64" fill="rgba(52, 152, 219, 0.3)" />
                    <text x="160" y="230" font-size="20" text-anchor="middle">‚Üí</text>
                </svg>
                <p><em>Figure 1: A 5x5 gridworld with start (S), goal (G), obstacles (black), and an example of a learned optimal path (blue with arrows).</em></p>
            </div>
            
            <h3>Reward Structure</h3>
            <p>To encourage the agent to find efficient paths, we define the following reward structure:</p>
            <ul>
                <li><strong>Goal reached:</strong> +1.0 (positive reward for reaching the goal)</li>
                <li><strong>Step penalty:</strong> -0.1 (small negative reward for each step to encourage shorter paths)</li>
                <li><strong>Wall collision:</strong> -0.5 (penalty for trying to move into a wall)</li>
            </ul>
            
            <p>This reward structure creates a learning challenge where the agent must balance immediate penalties (steps) against the future reward (reaching the goal).</p>
            
            <h3>The Q-Learning Algorithm</h3>
            
            <div class="algorithm-explanation">
                <h4>How Q-Learning Works</h4>
                <p>Q-learning is a value-based reinforcement learning algorithm that learns the value of taking a specific action in a specific state. It creates a Q-table that maps state-action pairs to expected future rewards.</p>
                
                <h5>Key Components:</h5>
                <ul>
                    <li><strong>Q-table:</strong> A table/dictionary that stores Q-values for each state-action pair</li>
                    <li><strong>Learning rate (Œ±):</strong> Controls how much new information overrides old information</li>
                    <li><strong>Discount factor (Œ≥):</strong> Determines how much the agent values future rewards</li>
                    <li><strong>Exploration rate (Œµ):</strong> Controls the balance between exploration and exploitation</li>
                </ul>
                
                <h5>The Q-Learning Update Rule:</h5>
                <div style="text-align: center; margin: 1.5rem 0; font-size: 1.2rem;">
                    Q(s,a) ‚Üê Q(s,a) + Œ± [r + Œ≥ max<sub>a'</sub> Q(s',a') - Q(s,a)]
                </div>
                
                <p>Where:</p>
                <ul>
                    <li><strong>Q(s,a)</strong> is the current Q-value for state s and action a</li>
                    <li><strong>r</strong> is the reward received after taking action a in state s</li>
                    <li><strong>s'</strong> is the new state after taking action a</li>
                    <li><strong>max<sub>a'</sub> Q(s',a')</strong> is the maximum Q-value for the next state s'</li>
                    <li><strong>Œ±</strong> is the learning rate</li>
                    <li><strong>Œ≥</strong> is the discount factor</li>
                </ul>
            </div>
            
            <h3>Exploration vs. Exploitation</h3>
            <p>A crucial aspect of reinforcement learning is balancing exploration (trying new actions to discover better rewards) with exploitation (using known good actions to maximize reward).</p>
            
            <div class="environment-diagram">
                <svg width="500" height="250" viewBox="0 0 500 250">
                    <!-- Exploration side -->
                    <rect x="50" y="50" width="150" height="150" rx="10" fill="#3498db" fill-opacity="0.3" stroke="#3498db" stroke-width="2"/>
                    <text x="125" y="40" font-size="16" text-anchor="middle" fill="#3498db">Exploration</text>
                    <text x="125" y="110" font-size="14" text-anchor="middle">Try random actions</text>
                    <text x="125" y="140" font-size="14" text-anchor="middle">Discover new paths</text>
                    <text x="125" y="170" font-size="14" text-anchor="middle">Learn environment</text>
                    
                    <!-- Exploitation side -->
                    <rect x="300" y="50" width="150" height="150" rx="10" fill="#e74c3c" fill-opacity="0.3" stroke="#e74c3c" stroke-width="2"/>
                    <text x="375" y="40" font-size="16" text-anchor="middle" fill="#e74c3c">Exploitation</text>
                    <text x="375" y="110" font-size="14" text-anchor="middle">Use best known actions</text>
                    <text x="375" y="140" font-size="14" text-anchor="middle">Take shortest paths</text>
                    <text x="375" y="170" font-size="14" text-anchor="middle">Maximize reward</text>
                    
                    <!-- Balance in the middle -->
                    <circle cx="250" cy="125" r="30" fill="#f39c12" fill-opacity="0.3" stroke="#f39c12" stroke-width="2"/>
                    <text x="250" y="120" font-size="14" text-anchor="middle" fill="#f39c12">Balance</text>
                    <text x="250" y="140" font-size="14" text-anchor="middle" fill="#f39c12">Œµ-greedy</text>
                    
                    <!-- Arrows -->
                    <path d="M175,125 L220,125" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
                    <path d="M280,125 L325,125" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
                    
                    <defs>
                        <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                            <polygon points="0 0, 10 3.5, 0 7" />
                        </marker>
                    </defs>
                </svg>
                <p><em>Figure 2: The exploration-exploitation tradeoff in reinforcement learning.</em></p>
            </div>
            
            <p>We'll use an <strong>Œµ-greedy policy</strong> to balance exploration and exploitation:</p>
            <ul>
                <li>With probability Œµ, choose a random action (exploration)</li>
                <li>With probability 1-Œµ, choose the action with the highest Q-value (exploitation)</li>
            </ul>
            
            <h3>Implementation Approach</h3>
            <p>The implementation of our gridworld Q-learning agent will follow these key steps:</p>
            
            <div class="implementation-steps">
                <div class="step">
                    <h4>Define the Environment</h4>
                    <p>First, we create a gridworld environment that the agent will interact with. This involves:</p>
                    <ul>
                        <li>Defining the grid size and layout</li>
                        <li>Setting up the walls/obstacles</li>
                        <li>Defining start and goal positions</li>
                        <li>Implementing the state transition dynamics</li>
                        <li>Defining the reward function</li>
                    </ul>
                    <p>The environment needs to provide methods for the agent to:</p>
                    <ul>
                        <li><strong>reset()</strong>: Reset the environment to the initial state</li>
                        <li><strong>step(action)</strong>: Take an action and return the next state, reward, and whether the episode has ended</li>
                        <li><strong>render()</strong>: Visualize the current state of the environment</li>
                    </ul>
                </div>
                
                <div class="step">
                    <h4>Initialize the Q-Learning Algorithm</h4>
                    <p>Next, we set up the Q-learning algorithm components:</p>
                    <ul>
                        <li>Initialize an empty Q-table (a dictionary mapping state-action pairs to Q-values)</li>
                        <li>Set learning parameters:
                            <ul>
                                <li>Learning rate (Œ±) = 0.1</li>
                                <li>Discount factor (Œ≥) = 0.99</li>
                                <li>Exploration rate (Œµ) = 0.1</li>
                            </ul>
                        </li>
                    </ul>
                </div>
                
                <div class="step">
                    <h4>Training Loop</h4>
                    <p>The training loop involves running multiple episodes where the agent interacts with the environment:</p>
                    <ol>
                        <li>Reset the environment to start a new episode</li>
                        <li>Choose actions using the Œµ-greedy policy:
                            <ul>
                                <li>With probability Œµ, select a random action (exploration)</li>
                                <li>With probability 1-Œµ, select the action with the highest Q-value (exploitation)</li>
                            </ul>
                        </li>
                        <li>Take the selected action, observe the next state and reward</li>
                        <li>Update the Q-value using the Q-learning update rule</li>
                        <li>Repeat until the episode ends (goal reached or maximum steps exceeded)</li>
                        <li>Start a new episode until the training is complete</li>
                    </ol>
                </div>
                
                <div class="step">
                    <h4>Policy Visualization</h4>
                    <p>After training, we need to visualize what the agent has learned:</p>
                    <ul>
                        <li>Extract the policy from the Q-table (best action for each state)</li>
                        <li>Visualize the gridworld with arrows indicating the optimal action in each cell</li>
                        <li>Trace the optimal path from start to goal</li>
                        <li>Analyze how the agent navigates around obstacles</li>
                    </ul>
                </div>
                
                <div class="step">
                    <h4>Evaluation and Analysis</h4>
                    <p>Finally, we analyze the agent's performance:</p>
                    <ul>
                        <li>Run test episodes using the learned policy (no exploration)</li>
                        <li>Measure statistics like:
                            <ul>
                                <li>Average number of steps to reach the goal</li>
                                <li>Success rate</li>
                                <li>Average reward per episode</li>
                            </ul>
                        </li>
                        <li>Experiment with different hyperparameters to understand their effects</li>
                    </ul>
                </div>
            </div>
            
            <!-- Q-Learning Algorithm Pseudocode -->
            <div class="algorithm-explanation">
                <h4>Q-Learning Algorithm Pseudocode</h4>
                <div class="code-concept">
            <pre style="background-color: #1e1e1e; color: #d4d4d4; padding: 15px; border-radius: 5px; overflow-x: auto;">
            <span style="color: #569cd6;">Algorithm:</span> Q-Learning for Gridworld Pathfinding

            <span style="color: #569cd6;">// Initialize environment and parameters</span>
            Initialize Gridworld with start, goal, obstacles
            Initialize Q(s,a) = 0 for all state-action pairs
            Set learning_rate Œ± = 0.1
            Set discount_factor Œ≥ = 0.99
            Set initial exploration_rate Œµ = 1.0
            Set exploration_min = 0.01
            Set exploration_decay = 0.995
            Set max_episodes = 1000
            Set max_steps = 100

            <span style="color: #569cd6;">// Training loop</span>
            For episode = 1 to max_episodes:
                Initialize state s = start_position
                
                For step = 1 to max_steps:
                    // Choose action using Œµ-greedy policy
                    With probability Œµ:
                        Choose action a randomly from possible actions
                    Otherwise:
                        Choose action a = argmax_a' Q(s, a')
                    
                    // Take action, observe reward and next state
                    Take action a, observe next state s' and reward r
                    
                    // Update Q-value using the Q-learning update rule
                    Q(s,a) = Q(s,a) + Œ± * [r + Œ≥ * max_a' Q(s',a') - Q(s,a)]
                    
                    // Move to next state
                    s = s'
                    
                    // If goal reached or maximum steps taken, end episode
                    If s is goal or step = max_steps:
                        Break
                
                // Decay exploration rate
                Œµ = max(exploration_min, Œµ * exploration_decay)
                
                // Track performance metrics
                track_episode_length(steps)
                track_episode_reward(total_reward)

            <span style="color: #569cd6;">// Extract policy from Q-values</span>
            For each state s:
                policy(s) = argmax_a Q(s,a)

            <span style="color: #569cd6;">// Visualization and evaluation</span>
            Visualize policy on grid with arrows
            Run test episodes following the learned policy
            Calculate average steps to goal
            Calculate success rate
            </pre>
                </div>
            </div>

            <!-- Policy Visualization Pseudocode -->
            <div class="algorithm-explanation">
                <h4>Policy Visualization Pseudocode</h4>
                <div class="code-concept">
            <pre style="background-color: #1e1e1e; color: #d4d4d4; padding: 15px; border-radius: 5px; overflow-x: auto;">
            <span style="color: #569cd6;">Function:</span> visualize_policy(Q_table, grid_size, start, goal, obstacles):
                // Create a representation of the grid for visualization
                grid = initialize_empty_grid(grid_size)
                
                // Mark special cells
                grid[start] = "S"
                grid[goal] = "G"
                For each obstacle in obstacles:
                    grid[obstacle] = "#"
                
                // Add arrows representing the optimal policy
                For each state s in grid:
                    If s is not start and s is not goal and s is not obstacle:
                        best_action = argmax_a Q_table[s, a]
                        If best_action == UP:
                            grid[s] = "‚Üë"
                        Else If best_action == RIGHT:
                            grid[s] = "‚Üí"
                        Else If best_action == DOWN:
                            grid[s] = "‚Üì"
                        Else If best_action == LEFT:
                            grid[s] = "‚Üê"
                
                // Highlight the optimal path from start to goal
                current = start
                path = [current]
                
                While current != goal and len(path) < grid_size * grid_size:
                    best_action = argmax_a Q_table[current, a]
                    next_state = get_next_state(current, best_action)
                    path.append(next_state)
                    current = next_state
                
                // Color cells in the optimal path
                For each state in path:
                    highlight_cell(grid, state)
                
                // Draw the grid with policy arrows and highlighted path
                draw_grid(grid)
                
                // Display additional metrics
                display_metrics(avg_steps, success_rate, avg_reward)
            </pre>
                </div>
            </div>

            <h3>Expected Results</h3>
            <p>After successful training, your agent should be able to:</p>
            <ul>
                <li>Find the shortest path from start to goal</li>
                <li>Successfully navigate around obstacles</li>
                <li>Maximize cumulative rewards</li>
            </ul>
            
            <p>A well-trained agent will show a policy that clearly indicates the optimal direction to move from each cell, leading to the goal along the shortest possible path.</p>
            
            <div class="environment-diagram">
                <svg width="500" height="300" viewBox="0 0 500 300">
                    <!-- Learning progress visualization -->
                    <!-- Early policy (after few episodes) -->
                    <g transform="translate(50, 50)">
                        <rect width="150" height="150" fill="white" stroke="#ccc" />
                        
                        <!-- Grid lines -->
                        <line x1="0" y1="50" x2="150" y2="50" stroke="#ccc" />
                        <line x1="0" y1="100" x2="150" y2="100" stroke="#ccc" />
                        <line x1="50" y1="0" x2="50" y2="150" stroke="#ccc" />
                        <line x1="100" y1="0" x2="100" y2="150" stroke="#ccc" />
                        
                        <!-- Start and goal -->
                        <rect x="0" y="0" width="50" height="50" fill="#FFD700" />
                        <text x="25" y="30" font-size="16" text-anchor="middle">S</text>
                        
                        <rect x="100" y="100" width="50" height="50" fill="#2ecc71" />
                        <text x="125" y="130" font-size="16" text-anchor="middle">G</text>
                        
                        <!-- Walls -->
                        <rect x="50" y="50" width="50" height="50" fill="#333" />
                        
                        <!-- Random arrows (early policy) -->
                        <text x="25" y="80" font-size="16" text-anchor="middle">‚Üì</text>
                        <text x="25" y="130" font-size="16" text-anchor="middle">‚Üê</text>
                        <text x="75" y="30" font-size="16" text-anchor="middle">‚Üí</text>
                        <text x="125" y="30" font-size="16" text-anchor="middle">‚Üë</text>
                        <text x="75" y="130" font-size="16" text-anchor="middle">‚Üí</text>
                        <text x="125" y="80" font-size="16" text-anchor="middle">‚Üì</text>
                    </g>
                    
                    <text x="125" y="220" font-size="14" text-anchor="middle">Early Training</text>
                    <text x="125" y="240" font-size="14" text-anchor="middle">(Random Policy)</text>
                    
                    <!-- Final policy (after many episodes) -->
                    <g transform="translate(300, 50)">
                        <rect width="150" height="150" fill="white" stroke="#ccc" />
                        
                        <!-- Grid lines -->
                        <line x1="0" y1="50" x2="150" y2="50" stroke="#ccc" />
                        <line x1="0" y1="100" x2="150" y2="100" stroke="#ccc" />
                        <line x1="50" y1="0" x2="50" y2="150" stroke="#ccc" />
                        <line x1="100" y1="0" x2="100" y2="150" stroke="#ccc" />
                        
                        <!-- Start and goal -->
                        <rect x="0" y="0" width="50" height="50" fill="#FFD700" />
                        <text x="25" y="30" font-size="16" text-anchor="middle">S</text>
                        
                        <rect x="100" y="100" width="50" height="50" fill="#2ecc71" />
                        <text x="125" y="130" font-size="16" text-anchor="middle">G</text>
                        
                        <!-- Walls -->
                        <rect x="50" y="50" width="50" height="50" fill="#333" />
                        
                        <!-- Optimal path (highlighted cells) -->
                        <rect x="0" y="50" width="50" height="50" fill="rgba(52, 152, 219, 0.3)" />
                        <rect x="0" y="100" width="50" height="50" fill="rgba(52, 152, 219, 0.3)" />
                        <rect x="50" y="100" width="50" height="50" fill="rgba(52, 152, 219, 0.3)" />
                        
                        <!-- Optimal policy arrows -->
                        <text x="25" y="30" font-size="16" text-anchor="middle">‚Üì</text>
                        <text x="25" y="80" font-size="16" text-anchor="middle">‚Üì</text>
                        <text x="25" y="130" font-size="16" text-anchor="middle">‚Üí</text>
                        <text x="75" y="30" font-size="16" text-anchor="middle">‚Üì</text>
                        <text x="75" y="130" font-size="16" text-anchor="middle">‚Üí</text>
                        <text x="125" y="30" font-size="16" text-anchor="middle">‚Üì</text>
                        <text x="125" y="80" font-size="16" text-anchor="middle">‚Üì</text>
                    </g>
                    
                    <text x="375" y="220" font-size="14" text-anchor="middle">After Training</text>
                    <text x="375" y="240" font-size="14" text-anchor="middle">(Optimal Policy)</text>
                </svg>
                <p><em>Figure 3: Comparison between early training (random policy) and after training (optimal policy).</em></p>
            </div>
            
            <h3>Common Challenges and Solutions</h3>
            
            <div class="implementation-steps">
                <div class="step">
                    <h4>Slow Convergence</h4>
                    <p><strong>Challenge:</strong> The agent takes too many episodes to learn an optimal policy.</p>
                    <p><strong>Solutions:</strong></p>
                    <ul>
                        <li>Adjust learning rate (try increasing Œ± for faster updates)</li>
                        <li>Implement a decaying exploration rate (high exploration early, more exploitation later)</li>
                        <li>Ensure the reward structure provides clear signals</li>
                        <li>Use a higher discount factor (Œ≥) to value future rewards more</li>
                    </ul>
                </div>
                
                <div class="step">
                    <h4>Suboptimal Paths</h4>
                    <p><strong>Challenge:</strong> The agent learns a path that reaches the goal but isn't the shortest.</p>
                    <p><strong>Solutions:</strong></p>
                    <ul>
                        <li>Increase the step penalty to discourage longer paths</li>
                        <li>Run more training episodes</li>
                        <li>Check for reward function issues or environment setup problems</li>
                        <li>Verify that the state representation captures all relevant information</li>
                    </ul>
                </div>
                
                <div class="step">
                    <h4>Exploration-Exploitation Balance</h4>
                    <p><strong>Challenge:</strong> The agent either explores too much (random behavior) or exploits too early (gets stuck in suboptimal paths).</p>
                    <p><strong>Solutions:</strong></p>
                    <ul>
                        <li>Implement a decaying exploration rate (Œµ)</li>
                        <li>Start with a high Œµ (e.g., 1.0) and gradually decrease to a low value (e.g., 0.01)</li>
                        <li>Use a schedule that ensures enough exploration early on</li>
                    </ul>
                </div>
            </div>
            
            <h3>Complete Implementation</h3>
            <p>For the complete implementation of this project, check out our GitHub repository. The code includes:</p>
            <ul>
                <li>Fully implemented gridworld environment</li>
                <li>Q-learning algorithm implementation</li>
                <li>Visualization utilities</li>
                <li>Hyperparameter experimentation tools</li>
                <li>Example results and analysis</li>
            </ul>
            
            <a href="https://github.com/EngineerProjects/Deep-Reinforcement-Learning/tree/main/01_Gridworld_Agent" target="_blank" class="github-link">
                <svg height="20" viewBox="0 0 16 16" width="20" fill="currentColor">
                    <path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                </svg>
                View Project on GitHub
            </a>
            
            <h3>Extensions and Further Exploration</h3>
            <p>Once you've successfully implemented the basic gridworld agent, consider these extensions to deepen your understanding:</p>
            
            <div class="note">
                <p><strong>Potential Extensions:</strong></p>
                <ul>
                    <li><strong>Stochastic Environment:</strong> Add uncertainty to the environment by making actions sometimes fail or have random outcomes</li>
                    <li><strong>Dynamic Obstacles:</strong> Add moving obstacles that the agent must learn to avoid</li>
                    <li><strong>Alternative Algorithms:</strong> Implement SARSA or Expected SARSA and compare with Q-learning</li>
                    <li><strong>Larger Gridworlds:</strong> Scale up to larger environments with more complex obstacle patterns</li>
                    <li><strong>Multiple Goals:</strong> Add multiple goals with different rewards to explore prioritization</li>
                    <li><strong>Visualization Tools:</strong> Create tools to visualize Q-values and learning progress over time</li>
                </ul>
            </div>
            
            <h3>Conclusion</h3>
            <p>This gridworld project provides a solid foundation for understanding reinforcement learning concepts. By implementing and experimenting with Q-learning in a simple environment, you'll gain insights into:</p>
            <ul>
                <li>How agents learn through trial and error</li>
                <li>The importance of reward design</li>
                <li>The exploration-exploitation tradeoff</li>
                <li>How value functions guide optimal decision-making</li>
            </ul>
            
            <p>These concepts form the building blocks for more advanced reinforcement learning algorithms and applications that you'll explore in later projects.</p>
        </section>
    </main>
    
    <footer>
        <p>&copy; 2025 Deep Reinforcement Learning Course</p>
        <p><a href="../applications.html" style="color: var(--light);">Back to Applications</a> | <a href="../../index.html" style="color: var(--light);">Back to Home</a></p>
    </footer>
</body>
</html>