<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deep Reinforcement Learning: Q-Learning</title>
    <link rel="stylesheet" href="../static/drl-course-css.css">
</head>
<body>
    <header>
        <h1>Deep Reinforcement Learning: Foundations</h1>
        <p>A comprehensive guide to Q-Learning and reinforcement learning fundamentals</p>
    </header>
    
    <nav>
        <ul>
            <li><a href="../index.html">Home</a></li>
            <li class="dropdown">
                <a href="Q-learning.html" class="active">Q-Learning</a>
                <ul class="submenu">
                    <li><a href="#introduction">Introduction</a></li>
                    <li><a href="#rl-basics">RL Basics</a></li>
                    <li><a href="#q-learning">Q-Learning Algorithm</a></li>
                </ul>
            </li>
            <li><a href="deep-q-learning.html">Deep Q-Learning</a></li>
            <li><a href="applications.html">Applications</a></li>
        </ul>
    </nav>
    
    <div style="text-align: center; margin: 2rem 0;">
        <a href="applications.html" class="button">Explore Q-learning Applications</a>
        <a href="https://github.com/EngineerProjects/Deep-Reinforcement-Learning" target="_blank" class="github-link" style="margin-left: 1rem;">
            <svg height="20" viewBox="0 0 16 16" width="20" fill="currentColor">
                <path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
            </svg>
            View Code Examples
        </a>
    </div>

    <main>
        <section id="introduction">
            <h2>Introduction to Reinforcement Learning</h2>
            
            <p>Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize cumulative reward. Unlike supervised learning, the agent is not explicitly told which actions to take but must discover which actions yield the highest reward through trial and error.</p>
            
            <div class="diagram">
                <svg width="600" height="300" viewBox="0 0 600 300">
                    <!-- Environment -->
                    <rect x="300" y="30" width="240" height="120" rx="10" fill="#ecf0f1" stroke="#2c3e50" stroke-width="2"/>
                    <text x="420" y="85" font-size="20" text-anchor="middle" fill="#2c3e50">Environment</text>
                    
                    <!-- Agent -->
                    <rect x="60" y="160" width="240" height="120" rx="10" fill="#FFD700" stroke="#E6C200" stroke-width="2"/>
                    <text x="180" y="220" font-size="20" text-anchor="middle" fill="#2c3e50">Agent</text>
                    
                    <!-- Action Arrow -->
                    <path d="M240,160 L360,90" stroke="#e74c3c" stroke-width="3" fill="none" marker-end="url(#arrowhead)"/>
                    <text x="300" y="90" font-size="16" text-anchor="middle" fill="#e74c3c">Action</text>
                    
                    <!-- State & Reward Arrow -->
                    <path d="M360,150 L240,200" stroke="#3498db" stroke-width="3" fill="none" marker-end="url(#arrowhead)"/>
                    <text x="290" y="190" font-size="16" text-anchor="middle" fill="#3498db">State, Reward</text>
                    
                    <!-- Arrow definition -->
                    <defs>
                        <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                            <polygon points="0 0, 10 3.5, 0 7" />
                        </marker>
                    </defs>
                </svg>
                <div class="diagram-caption">Figure 1: The Reinforcement Learning Framework</div>
            </div>
            
            <h3>Key Components of Reinforcement Learning</h3>
            
            <ul>
                <li><strong>Agent:</strong> The learner or decision-maker that interacts with the environment.</li>
                <li><strong>Environment:</strong> The world with which the agent interacts and learns from.</li>
                <li><strong>State (S):</strong> A specific configuration or situation in the environment.</li>
                <li><strong>Action (A):</strong> A decision the agent makes in a given state.</li>
                <li><strong>Reward (R):</strong> Feedback from the environment after the agent takes an action.</li>
                <li><strong>Policy (π):</strong> The agent's strategy or mapping from states to actions.</li>
                <li><strong>Value Function (V):</strong> An estimate of future rewards from a given state.</li>
                <li><strong>Q-Function (Q):</strong> An estimate of future rewards from a given state-action pair.</li>
            </ul>
            
            <h3>The RL Process</h3>
            
            <p>The reinforcement learning process follows a cycle:</p>
            
            <ol>
                <li>The agent observes the current state of the environment.</li>
                <li>Based on this state, the agent selects an action according to its policy.</li>
                <li>The environment transitions to a new state based on the agent's action.</li>
                <li>The environment provides a reward signal to the agent.</li>
                <li>The agent updates its policy or value function based on this new information.</li>
                <li>The process repeats, with the agent aiming to maximize cumulative rewards over time.</li>
            </ol>
        </section>
        
        <section id="rl-basics">
            <h2>Reinforcement Learning Basics</h2>
            
            <h3>Markov Decision Processes (MDPs)</h3>
            
            <p>Most reinforcement learning problems are modeled as Markov Decision Processes (MDPs). An MDP is defined by:</p>
            
            <ul>
                <li>A set of states S</li>
                <li>A set of actions A</li>
                <li>A transition probability function P(s'|s,a)</li>
                <li>A reward function R(s,a,s')</li>
                <li>A discount factor γ (gamma) ∈ [0,1]</li>
            </ul>
            
            <p>The Markov property states that the future depends only on the current state and action, not on the history of how we got there.</p>
            
            <div class="formula">
                P(s<sub>t+1</sub> | s<sub>t</sub>, a<sub>t</sub>, s<sub>t-1</sub>, a<sub>t-1</sub>, ..., s<sub>0</sub>, a<sub>0</sub>) = P(s<sub>t+1</sub> | s<sub>t</sub>, a<sub>t</sub>)
            </div>
            
            <h3>Rewards and Returns</h3>
            
            <p>The goal in RL is to maximize the expected cumulative discounted reward, called the return:</p>
            
            <div class="formula">
                G<sub>t</sub> = R<sub>t+1</sub> + γR<sub>t+2</sub> + γ<sup>2</sup>R<sub>t+3</sub> + ... = Σ<sub>k=0</sub><sup>∞</sup> γ<sup>k</sup>R<sub>t+k+1</sub>
            </div>
            
            <p>Where:</p>
            <ul>
                <li><strong>G<sub>t</sub></strong> is the return at time t</li>
                <li><strong>R<sub>t</sub></strong> is the reward at time t</li>
                <li><strong>γ</strong> (gamma) is the discount factor</li>
            </ul>
            
            <div class="note">
                <strong>Why discount rewards?</strong> Discounting (γ < 1) has several benefits:
                <ul>
                    <li>It mathematically ensures that the sum of rewards is finite, even in infinite-horizon problems.</li>
                    <li>It represents the uncertainty about the future or the preference for immediate rewards.</li>
                    <li>It helps the learning algorithm converge more reliably.</li>
                </ul>
            </div>

            <div class="reward-diagram">
                <div class="reward-state" style="top: 50px; left: 100px; background-color: #FFD700;">S<sub>t</sub></div>
                <div class="reward-state" style="top: 50px; left: 300px; background-color: #ecf0f1;">S<sub>t+1</sub></div>
                <div class="reward-state" style="top: 50px; left: 500px; background-color: #ecf0f1;">S<sub>t+2</sub></div>
                
                <div class="reward-arrow" style="top: 50px; left: 140px; width: 120px;"></div>
                <div class="reward-arrow" style="top: 50px; left: 340px; width: 120px;"></div>
                
                <div style="position: absolute; top: 110px; left: 300px;">R<sub>t+1</sub></div>
                <div style="position: absolute; top: 110px; left: 500px;">R<sub>t+2</sub> × γ</div>
                
                <div style="position: absolute; top: 200px; left: 300px; text-align: center;">
                    G<sub>t</sub> = R<sub>t+1</sub> + γR<sub>t+2</sub> + γ<sup>2</sup>R<sub>t+3</sub> + ...
                </div>
            </div>
            
            <h3>Value Functions</h3>
            
            <p>Value functions estimate how good it is for an agent to be in a certain state or to take a specific action in a state. They are defined based on expected returns:</p>
            
            <h4>State-Value Function (V-function)</h4>
            
            <div class="formula">
                V<sup>π</sup>(s) = E<sub>π</sub>[G<sub>t</sub> | S<sub>t</sub> = s]
            </div>
            
            <p>This represents the expected return when starting in state s and following policy π thereafter.</p>
            
            <h4>Action-Value Function (Q-function)</h4>
            
            <div class="formula">
                Q<sup>π</sup>(s,a) = E<sub>π</sub>[G<sub>t</sub> | S<sub>t</sub> = s, A<sub>t</sub> = a]
            </div>
            
            <p>This represents the expected return when taking action a in state s, and following policy π thereafter.</p>
            
            <h3>The Bellman Equation</h3>
            
            <p>The Bellman equation is a fundamental recursive relationship that forms the basis for many RL algorithms:</p>
            
            <div class="formula">
                V<sup>π</sup>(s) = Σ<sub>a</sub> π(a|s) Σ<sub>s',r</sub> p(s',r|s,a)[r + γV<sup>π</sup>(s')]
            </div>
            
            <p>For the Q-function:</p>
            
            <div class="formula">
                Q<sup>π</sup>(s,a) = Σ<sub>s',r</sub> p(s',r|s,a)[r + γ Σ<sub>a'</sub> π(a'|s') Q<sup>π</sup>(s',a')]
            </div>
            
            <div class="diagram">
                <svg width="600" height="300" viewBox="0 0 600 300">
                    <!-- Current State -->
                    <circle cx="150" cy="150" r="60" fill="#FFD700" stroke="#E6C200" stroke-width="2"/>
                    <text x="150" y="155" font-size="20" text-anchor="middle" fill="#2c3e50">State s</text>
                    
                    <!-- Next State 1 -->
                    <circle cx="350" cy="80" r="40" fill="#ecf0f1" stroke="#2c3e50" stroke-width="2"/>
                    <text x="350" y="85" font-size="16" text-anchor="middle" fill="#2c3e50">State s'₁</text>
                    
                    <!-- Next State 2 -->
                    <circle cx="450" cy="150" r="40" fill="#ecf0f1" stroke="#2c3e50" stroke-width="2"/>
                    <text x="450" y="155" font-size="16" text-anchor="middle" fill="#2c3e50">State s'₂</text>
                    
                    <!-- Next State 3 -->
                    <circle cx="350" cy="220" r="40" fill="#ecf0f1" stroke="#2c3e50" stroke-width="2"/>
                    <text x="350" y="225" font-size="16" text-anchor="middle" fill="#2c3e50">State s'₃</text>
                    
                    <!-- Arrows -->
                    <path d="M200,130 L310,90" stroke="#3498db" stroke-width="2" fill="none" marker-end="url(#arrowhead2)"/>
                    <path d="M210,150 L410,150" stroke="#3498db" stroke-width="2" fill="none" marker-end="url(#arrowhead2)"/>
                    <path d="M200,170 L310,210" stroke="#3498db" stroke-width="2" fill="none" marker-end="url(#arrowhead2)"/>
                    
                    <!-- Probability Labels -->
                    <text x="255" y="95" font-size="14" fill="#2c3e50">p₁, r₁</text>
                    <text x="300" y="140" font-size="14" fill="#2c3e50">p₂, r₂</text>
                    <text x="255" y="205" font-size="14" fill="#2c3e50">p₃, r₃</text>
                    
                    <!-- Arrow definition -->
                    <defs>
                        <marker id="arrowhead2" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                            <polygon points="0 0, 10 3.5, 0 7" fill="#3498db" />
                        </marker>
                    </defs>
                </svg>
                <div class="diagram-caption">Figure 2: Visualizing the Bellman Equation - A state transitions to possible next states with associated probabilities and rewards</div>
            </div>
        </section>
        
        <section id="q-learning">
            <h2>Q-Learning Algorithm</h2>
            
            <p>Q-Learning is a fundamental reinforcement learning algorithm that learns the value of an action in a particular state. It's a model-free, off-policy algorithm that directly approximates the optimal action-value function.</p>
            
            <div class="note">
                <p><strong>Key Characteristics:</strong></p>
                <ul>
                    <li><strong>Model-free:</strong> It doesn't require a model of the environment's dynamics.</li>
                    <li><strong>Off-policy:</strong> It can learn from actions that are outside the current policy.</li>
                    <li><strong>Value-based:</strong> It estimates action values to derive a policy.</li>
                </ul>
            </div>
            
            <h3>The Q-Learning Update Rule</h3>
            
            <p>The core of Q-learning is the update rule:</p>
            
            <div class="formula">
                Q(s,a) ← Q(s,a) + α [r + γ max<sub>a'</sub> Q(s',a') - Q(s,a)]
            </div>
            
            <p>Where:</p>
            <ul>
                <li><strong>Q(s,a)</strong> is the current Q-value for state s and action a</li>
                <li><strong>α (alpha)</strong> is the learning rate (0 < α ≤ 1)</li>
                <li><strong>r</strong> is the reward received</li>
                <li><strong>γ (gamma)</strong> is the discount factor (0 ≤ γ ≤ 1)</li>
                <li><strong>max<sub>a'</sub> Q(s',a')</strong> is the maximum Q-value for the next state s'</li>
            </ul>
            
            <h3>Understanding the Update</h3>
            
            <p>The Q-learning update can be broken down into these components:</p>
            
            <ul>
                <li><strong>Current estimate:</strong> Q(s,a)</li>
                <li><strong>Temporal difference (TD) target:</strong> r + γ max<sub>a'</sub> Q(s',a')</li>
                <li><strong>TD error:</strong> r + γ max<sub>a'</sub> Q(s',a') - Q(s,a)</li>
                <li><strong>Update step:</strong> The TD error scaled by the learning rate α</li>
            </ul>
            
            <div class="diagram">
                <svg width="600" height="350" viewBox="0 0 600 350">
                    <!-- Current State and Action -->
                    <rect x="50" y="100" width="120" height="60" rx="10" fill="#FFD700" stroke="#E6C200" stroke-width="2"/>
                    <text x="110" y="135" font-size="18" text-anchor="middle" fill="#2c3e50">Q(s,a)</text>
                    
                    <!-- Reward -->
                    <circle cx="250" cy="130" r="40" fill="#e74c3c" stroke="#c0392b" stroke-width="2"/>
                    <text x="250" y="135" font-size="18" text-anchor="middle" fill="white">r</text>
                    
                    <!-- Next State -->
                    <rect x="350" y="100" width="120" height="60" rx="10" fill="#ecf0f1" stroke="#2c3e50" stroke-width="2"/>
                    <text x="410" y="135" font-size="18" text-anchor="middle" fill="#2c3e50">s'</text>
                    
                    <!-- Next Actions -->
                    <rect x="350" y="190" width="60" height="40" rx="5" fill="#3498db" stroke="#2980b9" stroke-width="2"/>
                    <text x="380" y="215" font-size="16" text-anchor="middle" fill="white">a'₁</text>
                    
                    <rect x="420" y="190" width="60" height="40" rx="5" fill="#3498db" stroke="#2980b9" stroke-width="2"/>
                    <text x="450" y="215" font-size="16" text-anchor="middle" fill="white">a'₂</text>
                    
                    <!-- Q-Values -->
                    <rect x="350" y="250" width="60" height="40" rx="5" fill="#2ecc71" stroke="#27ae60" stroke-width="2"/>
                    <text x="380" y="275" font-size="16" text-anchor="middle" fill="white">Q₁</text>
                    
                    <rect x="420" y="250" width="60" height="40" rx="5" fill="#2ecc71" stroke="#27ae60" stroke-width="2"/>
                    <text x="450" y="275" font-size="16" text-anchor="middle" fill="white">Q₂</text>
                    
                    <!-- Max Q -->
                    <rect x="500" y="220" width="80" height="50" rx="5" fill="#9b59b6" stroke="#8e44ad" stroke-width="2"/>
                    <text x="540" y="250" font-size="16" text-anchor="middle" fill="white">max Q</text>
                    
                    <!-- Arrows connecting components -->
                    <path d="M170,130 L210,130" stroke="#333" stroke-width="2" marker-end="url(#arrowhead3)"/>
                    <path d="M290,130 L330,130" stroke="#333" stroke-width="2" marker-end="url(#arrowhead3)"/>
                    <path d="M410,160 L410,190" stroke="#333" stroke-width="2" marker-end="url(#arrowhead3)"/>
                    <path d="M450,160 L450,190" stroke="#333" stroke-width="2" marker-end="url(#arrowhead3)"/>
                    <path d="M380,230 L380,250" stroke="#333" stroke-width="2" marker-end="url(#arrowhead3)"/>
                    <path d="M450,230 L450,250" stroke="#333" stroke-width="2" marker-end="url(#arrowhead3)"/>
                    <path d="M380,290 L500,290 L500,270" stroke="#333" stroke-width="2" marker-end="url(#arrowhead3)"/>
                    <path d="M450,290 L480,290" stroke="#333" stroke-width="2"/>
                    
                    <!-- TD Error -->
                    <rect x="180" y="280" width="240" height="50" rx="5" fill="#e74c3c" stroke="#c0392b" stroke-width="2"/>
                    <text x="300" y="312" font-size="16" text-anchor="middle" fill="white">TD Error = Target - Current</text>
                    
                    <!-- Arrow from max Q to TD Error -->
                    <path d="M500,270 L500,305 L420,305" stroke="#333" stroke-width="2" marker-end="url(#arrowhead3)"/>
                    
                    <!-- Arrow from Current Q to TD Error -->
                    <path d="M110,160 L110,305 L180,305" stroke="#333" stroke-width="2" marker-end="url(#arrowhead3)"/>
                    
                    <!-- Arrow from Reward to TD Error -->
                    <path d="M250,170 L250,280 L250,305" stroke="#333" stroke-width="2" marker-end="url(#arrowhead3)"/>
                    
                    <!-- Arrow definition -->
                    <defs>
                        <marker id="arrowhead3" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                            <polygon points="0 0, 10 3.5, 0 7" />
                        </marker>
                    </defs>
                </svg>
                <div class="diagram-caption">Figure 3: Visualization of the Q-Learning update process</div>
            </div>
            
            <h3>Q-Learning Algorithm Steps</h3>
            
            <div class="algorithm-box">
                <h4>Q-Learning Algorithm</h4>
                
                <p><strong>Initialize:</strong></p>
                <div class="algorithm-step">
                    <div class="step-number">1.</div>
                    <div>Initialize Q-table Q(s,a) arbitrarily for all state-action pairs (typically to zeros)</div>
                </div>
                
                <p><strong>For each episode:</strong></p>
                <div class="algorithm-step">
                    <div class="step-number">2.</div>
                    <div>Initialize state s</div>
                </div>
                
                <p><strong>For each step of the episode:</strong></p>
                <div class="algorithm-step">
                    <div class="step-number">3.</div>
                    <div>Choose action a from state s using policy derived from Q (e.g., ε-greedy)</div>
                </div>
                <div class="algorithm-step">
                    <div class="step-number">4.</div>
                    <div>Take action a, observe reward r and next state s'</div>
                </div>
                <div class="algorithm-step">
                    <div class="step-number">5.</div>
                    <div>Update Q(s,a) ← Q(s,a) + α[r + γ max<sub>a'</sub> Q(s',a') - Q(s,a)]</div>
                </div>
                <div class="algorithm-step">
                    <div class="step-number">6.</div>
                    <div>s ← s' (update current state)</div>
                </div>
                <div class="algorithm-step">
                    <div class="step-number">7.</div>
                    <div>If s is terminal, end episode</div>
                </div>
            </div>
            
            <h3>Exploration vs. Exploitation</h3>
            
            <p>A key challenge in reinforcement learning is balancing exploration (trying new actions to discover better rewards) with exploitation (using known good actions to maximize reward).</p>
            
            <div class="diagram">
                <svg width="600" height="300" viewBox="0 0 600 300">
                    <!-- Exploration side -->
                    <rect x="50" y="50" width="200" height="200" rx="10" fill="#3498db" fill-opacity="0.3" stroke="#3498db" stroke-width="2"/>
                    <text x="150" y="40" font-size="18" text-anchor="middle" fill="#3498db">Exploration</text>
                    <text x="150" y="150" font-size="14" text-anchor="middle">Try new actions</text>
                    <text x="150" y="180" font-size="14" text-anchor="middle">Discover better strategies</text>
                    <text x="150" y="210" font-size="14" text-anchor="middle">Random actions</text>
                    
                    <!-- Exploitation side -->
                    <rect x="350" y="50" width="200" height="200" rx="10" fill="#e74c3c" fill-opacity="0.3" stroke="#e74c3c" stroke-width="2"/>
                    <text x="450" y="40" font-size="18" text-anchor="middle" fill="#e74c3c">Exploitation</text>
                    <text x="450" y="150" font-size="14" text-anchor="middle">Use best known actions</text>
                    <text x="450" y="180" font-size="14" text-anchor="middle">Maximize immediate reward</text>
                    <text x="450" y="210" font-size="14" text-anchor="middle">Greedy selection</text>
                    
                    <!-- Balance in the middle -->
                    <circle cx="300" cy="150" r="50" fill="#f39c12" fill-opacity="0.3" stroke="#f39c12" stroke-width="2"/>
                    <text x="300" y="145" font-size="16" text-anchor="middle" fill="#f39c12">Balance</text>
                    <text x="300" y="165" font-size="16" text-anchor="middle" fill="#f39c12">ε-greedy</text>
                </svg>
                <div class="diagram-caption">Figure 4: The exploration-exploitation tradeoff in reinforcement learning</div>
            </div>
            
            <h4>ε-Greedy Policy</h4>
            
            <p>The most common approach to balance exploration and exploitation in Q-learning is the ε-greedy policy:</p>
            
            <ul>
                <li>With probability ε, choose a random action (exploration)</li>
                <li>With probability 1-ε, choose the action with the highest Q-value (exploitation)</li>
            </ul>
            
            <div class="pseudocode">
                <span class="keyword">FUNCTION</span> ChooseAction(state, qTable, epsilon):
                    <span class="keyword">IF</span> random() < epsilon <span class="keyword">THEN</span>
                        <span class="comment">// Exploration: choose a random action</span>
                        <span class="keyword">RETURN</span> randomChoice(availableActions)
                    <span class="keyword">ELSE</span>
                        <span class="comment">// Exploitation: choose the best action</span>
                        <span class="keyword">RETURN</span> argmax(qTable[state])
                    <span class="keyword">END IF</span>
                <span class="keyword">END FUNCTION</span>
            </div>
            
            <h4>Decaying Exploration Rate</h4>
            
            <p>It's common to start with a high exploration rate (high ε) and gradually decrease it as the agent learns:</p>
            
            <div class="pseudocode">
                <span class="comment">// After each episode or after a certain number of steps</span>
                epsilon = max(minEpsilon, epsilon * decayRate)
            </div>
            
            <div class="diagram">
                <svg width="600" height="300" viewBox="0 0 600 300">
                    <!-- Axes -->
                    <line x1="50" y1="250" x2="550" y2="250" stroke="#333" stroke-width="2"/>
                    <line x1="50" y1="50" x2="50" y2="250" stroke="#333" stroke-width="2"/>
                    
                    <!-- X-axis ticks -->
                    <line x1="50" y1="250" x2="50" y2="255" stroke="#333" stroke-width="2"/>
                    <line x1="200" y1="250" x2="200" y2="255" stroke="#333" stroke-width="2"/>
                    <line x1="350" y1="250" x2="350" y2="255" stroke="#333" stroke-width="2"/>
                    <line x1="500" y1="250" x2="500" y2="255" stroke="#333" stroke-width="2"/>
                    
                    <!-- Y-axis ticks -->
                    <line x1="45" y1="50" x2="50" y2="50" stroke="#333" stroke-width="2"/>
                    <line x1="45" y1="150" x2="50" y2="150" stroke="#333" stroke-width="2"/>
                    <line x1="45" y1="250" x2="50" y2="250" stroke="#333" stroke-width="2"/>
                    
                    <!-- Labels -->
                    <text x="300" y="280" font-size="14" text-anchor="middle">Training Progress (Episodes)</text>
                    <text x="30" y="150" font-size="14" text-anchor="middle" transform="rotate(-90, 30, 150)">Exploration Rate (ε)</text>
                    
                    <!-- Tick labels -->
                    <text x="50" y="270" font-size="12" text-anchor="middle">0</text>
                    <text x="200" y="270" font-size="12" text-anchor="middle">100</text>
                    <text x="350" y="270" font-size="12" text-anchor="middle">200</text>
                    <text x="500" y="270" font-size="12" text-anchor="middle">300</text>
                    
                    <text x="40" y="250" font-size="12" text-anchor="end">0</text>
                    <text x="40" y="150" font-size="12" text-anchor="end">0.5</text>
                    <text x="40" y="50" font-size="12" text-anchor="end">1.0</text>
                    
                    <!-- Decay curve -->
                    <path d="M50,50 C100,60 150,100 200,150 C250,200 300,230 500,245" stroke="#e74c3c" stroke-width="3" fill="none"/>
                    
                    <!-- Minimum epsilon line -->
                    <line x1="50" y1="240" x2="550" y2="240" stroke="#3498db" stroke-width="1" stroke-dasharray="5,5"/>
                    <text x="560" y="240" font-size="12" fill="#3498db">min ε</text>
                </svg>
                <div class="diagram-caption">Figure 5: Exploration rate decay over training episodes</div>
            </div>
            
            <div class="tip">
                <p><strong>Best Practice:</strong> Start with high exploration (ε ≈ 1.0) and decay to a small value (e.g., 0.01) to ensure the agent explores sufficiently at the beginning but exploits its knowledge later on.</p>
            </div>
            
            <h3>Learning Rate and Discount Factor</h3>
            
            <h4>Learning Rate (α)</h4>
            
            <p>The learning rate determines how much new information overrides old information:</p>
            <ul>
                <li><strong>α = 0:</strong> The agent doesn't learn anything (Q-values never update)</li>
                <li><strong>α = 1:</strong> The agent only considers the most recent information (ignores prior Q-values)</li>
                <li><strong>0 < α < 1:</strong> The agent balances new and old information</li>
            </ul>
            
            <div class="tip">
                <p>Typical values for α range from 0.1 to 0.5. A higher learning rate means faster learning but can lead to instability, while a lower learning rate leads to more stable but slower learning.</p>
            </div>
            
            <table>
                <tr>
                    <th>Learning Rate (α)</th>
                    <th>Advantages</th>
                    <th>Disadvantages</th>
                    <th>Best Used When</th>
                </tr>
                <tr>
                    <td>Low (0.01-0.1)</td>
                    <td>
                        <ul>
                            <li>Stable learning</li>
                            <li>Less sensitive to noise</li>
                            <li>Smooth convergence</li>
                        </ul>
                    </td>
                    <td>
                        <ul>
                            <li>Slow learning</li>
                            <li>May get stuck in local optima</li>
                        </ul>
                    </td>
                    <td>Environment is deterministic or has low noise</td>
                </tr>
                <tr>
                    <td>Medium (0.1-0.5)</td>
                    <td>
                        <ul>
                            <li>Balanced learning speed</li>
                            <li>Good general performance</li>
                        </ul>
                    </td>
                    <td>
                        <ul>
                            <li>Requires tuning</li>
                        </ul>
                    </td>
                    <td>Most general reinforcement learning problems</td>
                </tr>
                <tr>
                    <td>High (0.5-1.0)</td>
                    <td>
                        <ul>
                            <li>Fast learning</li>
                            <li>Quick adaptation to changes</li>
                        </ul>
                    </td>
                    <td>
                        <ul>
                            <li>Unstable learning</li>
                            <li>Sensitive to noise</li>
                            <li>May oscillate or diverge</li>
                        </ul>
                    </td>
                    <td>Non-stationary environments that change rapidly</td>
                </tr>
            </table>
            
            <h4>Discount Factor (γ)</h4>
            
            <p>As covered earlier, the discount factor determines how much the agent values future rewards:</p>
            <ul>
                <li><strong>γ = 0:</strong> Only immediate rewards matter</li>
                <li><strong>γ = 1:</strong> All future rewards are equally important as immediate rewards</li>
                <li><strong>0 < γ < 1:</strong> Future rewards are discounted (closer rewards are more important)</li>
            </ul>
            
            <div class="tip">
                <p>Typical values for γ range from 0.9 to 0.99, emphasizing the importance of long-term rewards while ensuring the sum of rewards remains finite.</p>
            </div>
            
            <h3>Convergence of Q-Learning</h3>
            
            <p>Under certain conditions, Q-learning is guaranteed to converge to the optimal Q-function:</p>
            
            <ul>
                <li>Every state-action pair must be visited infinitely often</li>
                <li>The learning rate must satisfy the Robbins-Monro conditions:
                    <ul>
                        <li>Σ<sub>t</sub> α<sub>t</sub> = ∞</li>
                        <li>Σ<sub>t</sub> α<sub>t</sub><sup>2</sup> < ∞</li>
                    </ul>
                </li>
                <li>The reward must be bounded</li>
            </ul>
            
            <div class="note">
                <p>In practice, these theoretical conditions are often relaxed, and Q-learning can work well even when they are not strictly met. The key is to ensure sufficient exploration and appropriate hyperparameter settings.</p>
            </div>
            
            <h3>Implementing Q-Learning in Practice</h3>
            
            <p>The implementation of Q-learning typically involves these key components:</p>
            
            <ol>
                <li><strong>Q-table:</strong> A data structure (usually a matrix or dictionary) that stores Q-values for each state-action pair</li>
                <li><strong>Action selection mechanism:</strong> Usually ε-greedy or another exploration strategy</li>
                <li><strong>Update rule:</strong> The Q-learning equation to update Q-values based on experience</li>
                <li><strong>Hyperparameter management:</strong> Setting and potentially decaying parameters like learning rate and exploration rate</li>
            </ol>
            
            <div class="pseudocode">
                <span class="keyword">ALGORITHM</span> Q-Learning
                    <span class="comment">// Initialize Q-table</span>
                    <span class="keyword">FOR EACH</span> state s <span class="keyword">DO</span>
                        <span class="keyword">FOR EACH</span> action a <span class="keyword">DO</span>
                            Q(s, a) ← 0
                        <span class="keyword">END FOR</span>
                    <span class="keyword">END FOR</span>
    
    <span class="comment">// Learning parameters</span>
    alpha ← learning rate (e.g., 0.1)
    gamma ← discount factor (e.g., 0.95)
    epsilon ← initial exploration rate (e.g., 1.0)
    minEpsilon ← minimum exploration rate (e.g., 0.01)
    decayRate ← exploration decay rate (e.g., 0.995)
        
        <span class="comment">// Training loop</span>
    <span class="keyword">FOR</span> episode = 1 to numEpisodes <span class="keyword">DO</span>
        s ← initialState
        
        <span class="keyword">WHILE</span> s is not terminal <span class="keyword">DO</span>
            <span class="comment">// Choose action using epsilon-greedy</span>
            <span class="keyword">IF</span> random() < epsilon <span class="keyword">THEN</span>
                a ← randomAction
            <span class="keyword">ELSE</span>
                a ← argmax_a' Q(s, a')
            <span class="keyword">END IF</span>
            
            <span class="comment">// Take action, observe reward and next state</span>
            (s', r) ← takeAction(s, a)
            
            <span class="comment">// Update Q-value</span>
            maxFutureQ ← max_a' Q(s', a')
            Q(s, a) ← Q(s, a) + alpha * (r + gamma * maxFutureQ - Q(s, a))
            
            s ← s'
        <span class="keyword">END WHILE</span>
        
        <span class="comment">// Decay exploration rate</span>
        epsilon ← max(minEpsilon, epsilon * decayRate)
    <span class="keyword">END FOR</span>
<span class="keyword">END ALGORITHM</span>
            </div>
            
            <h3>Applications of Q-Learning</h3>
            
            <p>Q-learning has been successfully applied to a wide range of domains, including:</p>
            
            <ul>
                <li><strong>Game playing:</strong> Teaching agents to play games like Atari, board games, etc.</li>
                <li><strong>Robotics:</strong> Teaching robots to navigate, manipulate objects, etc.</li>
                <li><strong>Resource management:</strong> Optimizing resource allocation in computing systems, networks, etc.</li>
                <li><strong>Finance:</strong> Trading strategies, portfolio management, etc.</li>
                <li><strong>Healthcare:</strong> Treatment optimization, personalized medicine, etc.</li>
            </ul>
            
            <div class="note">
                <p>While basic Q-learning works well for smaller state spaces, it can struggle with large or continuous spaces. Deep Q-Networks (DQN) extend Q-learning with neural networks to handle these more complex environments.</p>
            </div>
        </section>

        <div style="text-align: center; margin: 2rem 0;">
            <a href="applications.html" class="button">Explore Q-learning Applications</a>
            <a href="https://github.com/EngineerProjects/Deep-Reinforcement-Learning" target="_blank" class="github-link" style="margin-left: 1rem;">
                <svg height="20" viewBox="0 0 16 16" width="20" fill="currentColor">
                    <path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                </svg>
                View Code Examples
            </a>
        </div>
    </main>
    
    <footer>
        <p>&copy; 2025 Deep Reinforcement Learning Course</p>
        <p><a href="../index.html" style="color: var(--light);">Back to Home</a></p>
    </footer>
</body>
</html>